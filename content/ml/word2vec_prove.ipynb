{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec formula prove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0)\n",
    "\n",
    "$$\n",
    "\\sigma' (y) = \\sigma (y) \\cdot [ 1 - \\sigma (y) ] \\cdot y'\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hierarchical softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "$$ p(w | w_I) = \\prod_{j=1}^{L(w)-1} \\sigma \\left( [\\![ n(w, j+1) = ch(n(w, j)) ]\\!] \\cdot {v'_{n(w, j)}}^T v_{w_I} \n",
    "\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the hierarchical softmax gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let\n",
    "\n",
    "$$\n",
    "hs_j = [\\![ n(w, j+1) = ch(n(w, j)) ]\\!] \\cdot {v'_{n(w, j)}}^T v_{w_I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And\n",
    "\n",
    "$$\n",
    "\\frac {d hs_j} {v'_{n(w, j)}} = [\\![ n(w, j+1) = ch(n(w, j)) ]\\!] \\cdot v_{w_I}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {d hs_j} {v_{w_I}} = [\\![ n(w, j+1) = ch(n(w, j)) ]\\!] \\cdot v'_{n(w, j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "$$\n",
    "\\nabla \\log p(w | w_I) = \\sum_{j=1}^{L(w)-1} \\frac{1} {\\sigma (hs_j)} \\cdot \\sigma' (hs_j) \\cdot hs'_j\n",
    "$$\n",
    "\n",
    "According to equation (0)\n",
    "$$\n",
    "\\nabla \\log p(w | w_I) = \\sum_{j=1}^{L(w)-1}\n",
    "\\frac{1} {\\sigma (hs_j)}\n",
    "\\cdot\n",
    "\\sigma (hs_j) \\cdot [ 1 - \\sigma (hs_j) ]\n",
    "\\cdot\n",
    "hs'_j\n",
    "$$\n",
    "\n",
    "simplify\n",
    "$$\n",
    "\\nabla \\log p(w | w_I) = \\sum_{j=1}^{L(w)-1}\n",
    "[ 1 - \\sigma (hs_j) ] \\cdot hs'_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When update $ v_{w_I} $, we got gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(w | w_I) = \\sum_{j=1}^{L(w)-1}\n",
    "[ 1 - \\sigma (hs_j) ] \\cdot\n",
    "[\\![ n(w, j+1) = ch(n(w, j)) ]\\!]\n",
    "\\cdot v'_{n(w, j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When update $ v'_{n(w, j)} $, we got gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(w | w_I) = \\sum_{j=1}^{L(w)-1}\n",
    "[ 1 - \\sigma (hs_j) ]\n",
    "\\cdot\n",
    "[\\![ n(w, j+1) = ch(n(w, j)) ]\\!]\n",
    "\\cdot\n",
    "v_{w_I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the negative sampling gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)\n",
    "$$\n",
    "p(w_O | w_I) = \\log \\sigma( {v'_{w_O}}^T v_{w_I} ) + \n",
    "\\sum_{i}^{k} E_{w_i \\sim P_n(w)}\n",
    "\\left[\n",
    "\\log \\sigma( {-v'_{w_i}}^T v_{w_I} )\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let\n",
    "\n",
    "$$\n",
    "ns_{OI} = {v'_{w_O}}^T \\cdot v_{w_I}\n",
    "$$\n",
    "\n",
    "$$\n",
    "ns_{iI} = -{v'_{w_i}}^T \\cdot v_{w_I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And\n",
    "\n",
    "the gradients $ns_{OI}$ according to ${v'_{w_O}}$ and $v_{w_I}$ are:\n",
    "\n",
    "$$\n",
    "\\frac {d ns_{OI}} {v'_{w_O}} = v_{w_I}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {d ns_{OI}} {v_{w_I}} = v'_{w_O}\n",
    "$$\n",
    "\n",
    "the gradients $ns_{iI}$ according to ${v'_{w_i}}$ and $v_{w_I}$ are:\n",
    "\n",
    "$$\n",
    "\\frac {d ns_{iI}} {v'_{w_i}} = -v_{w_I}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {d ns_{iI}} {v_{w_I}} = -v'_{w_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "\n",
    "$$\n",
    "\\nabla p(w_O | w_I) = \\frac {1} { \\sigma(ns_{OI}) }\n",
    "\\cdot\n",
    "\\sigma'(ns_{OI})\n",
    "\\cdot\n",
    "ns'_{OI}\n",
    "+\n",
    "\\sum_{i}^{k} E_{w_i \\sim P_n(w)}\n",
    "\\left[\n",
    "\\frac {1} { \\sigma(ns_{iI}) } \\cdot\n",
    "\\sigma'( ns_{iI} ) \\cdot\n",
    "ns'_{iI}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "According to equation (0)\n",
    "\n",
    "$$\n",
    "\\nabla p(w_O | w_I) = \\frac {1} { \\sigma(ns_{OI}) }\n",
    "\\cdot\n",
    "\\sigma(ns_{OI}) \\cdot [ 1 - \\sigma(ns_{OI}) ]\n",
    "\\cdot\n",
    "ns'_{OI}\n",
    "+\n",
    "\\sum_{i}^{k} E_{w_i \\sim P_n(w)}\n",
    "\\left[\n",
    "\\frac {1} { \\sigma(ns_{iI}) } \\cdot\n",
    "\\sigma ( ns_{iI} ) \\cdot\n",
    "[ 1 - \\sigma(ns_{iI}) ] \\cdot\n",
    "ns'_{iI}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "simplify\n",
    "\n",
    "$$\n",
    "\\nabla p(w_O | w_I) = [ 1 - \\sigma(ns_{OI}) ] \\cdot ns'_{OI}\n",
    "+\n",
    "\\sum_{i}^{k} E_{w_i \\sim P_n(w)}\n",
    "\\left[\n",
    "[ 1 - \\sigma(ns_{iI}) ] \\cdot\n",
    "ns'_{iI}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When update $ v'_{w_O} $, we got gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla p(w_O | w_I) = [ 1 - \\sigma(ns_{OI}) ] \\cdot v_{w_I}\n",
    "$$\n",
    "\n",
    "When update $ v_{w_I} $, we got gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla p(w_O | w_I) = [ 1 - \\sigma(ns_{OI}) ] \\cdot v'_{w_O}\n",
    "+\n",
    "\\sum_{i}^{k} E_{w_i \\sim P_n(w)}\n",
    "\\left[\n",
    "[ 1 - \\sigma(ns_{iI}) ] \\cdot\n",
    "-v'_{w_i}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "When update $ v'_{w_i} $, we got gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla p(w_O | w_I) = \\sum_{i}^{k} E_{w_i \\sim P_n(w)}\n",
    "\\left[\n",
    "[ 1 - \\sigma(ns_{iI}) ] \\cdot\n",
    "-v_{w_I}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the gradient of $ \\log( p(w_O | w_I) )$ is:\n",
    "\n",
    "$$\n",
    "\\nabla \\log( p(w_O | w_I) ) = \\frac{1} {p(w_O | w_I)} \\cdot\n",
    "\\nabla p(w_O | w_I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continuous bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maximize the following average log probability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq{j} \\leq{c},j\\neq{0}} \\log{p(w_t | w_{t+j})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximize the following average log probability:\n",
    "\n",
    "(4)\n",
    "$$ \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq{j} \\leq{c},j\\neq{0}} \\log{p(w_{t+j} | w_t)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
